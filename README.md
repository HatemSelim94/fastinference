<img src="docs/logo.png" width="600" height="150"> <h1>Fastinference</h1>

[![Building docs](https://github.com/sbuschjaeger/fastinference/actions/workflows/docs.yml/badge.svg)](https://github.com/sbuschjaeger/fastinference/actions/workflows/docs.yml)
[![Running tests](https://github.com/sbuschjaeger/fastinference/actions/workflows/tests.yml/badge.svg)](https://github.com/sbuschjaeger/fastinference/actions/workflows/tests.yml)

Machine Learning + jinja2 = Fastinference. Fastinference is a machine learning model compiler specifically targeted to small, embedded systems. Moreover, we aim to provide an easily extensible framework for researchers and practitioners alike. The main idea behind FastInference is to statically compile models into source code files which are then compiled using a regular compiler. This two-step process has the advantage that there is no runtime environment required on the target system, and we can make use of system-specific instructions on the embedded system. To make this approach flexible and accessible we heavily rely on template instantiation managed by jinja2. Simply put, we provide a set of pre-programmed templates for different ML models for a target language/backend, and then FastInference will make sure that correct data types, instructions, etc. are used during template instantiation. In order to add new ML models and/or backend, you simply need to provide the core computations for model application as a jinja2 template.

If you are interested in deploying machine learning models for production then this project is probably not mature enough for you. For deep learning there are a ton of frameworks available such as [glow](https://github.com/pytorch/glow), [tensorflow-lite](https://www.tensorflow.org/), [ONNX Runtime](https://github.com/microsoft/onnxruntime), [NGraph](https://github.com/NervanaSystems/ngraph), [MACE](https://github.com/XiaoMi/mace), [NCNN](https://github.com/Tencent/ncnn), [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt), [OpenVINO Toolkit](https://github.com/openvinotoolkit/openvino) and probably more. 
For classical machine learning algorithm we have fewer, but still very capable model compiler such as [Hummingbird](https://github.com/microsoft/hummingbird) or 
[Treelite](https://github.com/dmlc/treelite).

If you are interested in quickly trying out more obscure implementations then this tool might be interesting for you. Fastinference supports the generation of Binarized Neural Networks and has been used in our ECML Paper "On-Site Gamma-Hadron Separation with Deep Learning on FPGAs". It also has been used for generating cache-friendly DT ensembles in our paper "Realization of Random Forest for Real-Time  Evaluation through Tree Framing". 